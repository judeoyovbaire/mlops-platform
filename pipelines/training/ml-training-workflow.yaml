# Argo Workflow Template for ML Training Pipeline
# Demonstrates: data loading, validation, feature engineering, training, and model registration
apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: ml-training-pipeline
  namespace: argo
  labels:
    app: mlops
    pipeline: training
spec:
  entrypoint: ml-pipeline
  serviceAccountName: argo-workflows-server

  # Security context for all pods in this workflow
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000

  # Pipeline parameters with defaults
  arguments:
    parameters:
      - name: dataset-url
        value: "https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv"
      - name: target-column
        value: "species"
      - name: model-name
        value: "iris-classifier"
      - name: mlflow-tracking-uri
        value: "http://mlflow.mlflow.svc.cluster.local:5000"
      - name: accuracy-threshold
        value: "0.9"
      - name: n-estimators
        value: "100"
      - name: max-depth
        value: "10"
      - name: test-size
        value: "0.2"
      - name: random-state
        value: "42"
      - name: model-alias
        value: "champion"

  # Artifact storage using MinIO
  artifactRepositoryRef:
    configMap: artifact-repositories
    key: default-artifact-repository

  templates:
    # Main pipeline DAG
    - name: ml-pipeline
      dag:
        tasks:
          - name: load-data
            template: load-data
            arguments:
              parameters:
                - name: dataset-url
                  value: "{{workflow.parameters.dataset-url}}"

          - name: validate-data
            template: validate-data
            dependencies: [load-data]
            arguments:
              artifacts:
                - name: input-data
                  from: "{{tasks.load-data.outputs.artifacts.output-data}}"

          - name: feature-engineering
            template: feature-engineering
            dependencies: [validate-data]
            arguments:
              parameters:
                - name: target-column
                  value: "{{workflow.parameters.target-column}}"
              artifacts:
                - name: input-data
                  from: "{{tasks.validate-data.outputs.artifacts.output-data}}"

          - name: train-model
            template: train-model
            dependencies: [feature-engineering]
            arguments:
              parameters:
                - name: target-column
                  value: "{{workflow.parameters.target-column}}"
                - name: model-name
                  value: "{{workflow.parameters.model-name}}"
                - name: mlflow-tracking-uri
                  value: "{{workflow.parameters.mlflow-tracking-uri}}"
                - name: n-estimators
                  value: "{{workflow.parameters.n-estimators}}"
                - name: max-depth
                  value: "{{workflow.parameters.max-depth}}"
                - name: test-size
                  value: "{{workflow.parameters.test-size}}"
                - name: random-state
                  value: "{{workflow.parameters.random-state}}"
              artifacts:
                - name: input-data
                  from: "{{tasks.feature-engineering.outputs.artifacts.output-data}}"
                - name: scaler
                  from: "{{tasks.feature-engineering.outputs.artifacts.scaler}}"

          - name: register-model
            template: register-model
            dependencies: [train-model]
            arguments:
              parameters:
                - name: model-name
                  value: "{{workflow.parameters.model-name}}"
                - name: mlflow-tracking-uri
                  value: "{{workflow.parameters.mlflow-tracking-uri}}"
                - name: accuracy-threshold
                  value: "{{workflow.parameters.accuracy-threshold}}"
                - name: model-alias
                  value: "{{workflow.parameters.model-alias}}"
                - name: run-id
                  value: "{{tasks.train-model.outputs.parameters.run-id}}"

    # Step 1: Load data from URL
    - name: load-data
      retryStrategy:
        limit: 2
        retryPolicy: "OnError"
      inputs:
        parameters:
          - name: dataset-url
      outputs:
        artifacts:
          - name: output-data
            path: /tmp/data.csv
      container:
        image: python:3.10-slim
        command: [python, /src/load_data.py]
        args:
          - --url
          - "{{inputs.parameters.dataset-url}}"
          - --output
          - /tmp/data.csv
        volumeMounts:
          - name: scripts
            mountPath: /src
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
      volumes:
        - name: scripts
          configMap:
            name: ml-training-scripts

    # Step 2: Validate data quality
    - name: validate-data
      retryStrategy:
        limit: 2
        retryPolicy: "OnError"
      inputs:
        artifacts:
          - name: input-data
            path: /tmp/input.csv
      outputs:
        artifacts:
          - name: output-data
            path: /tmp/validated.csv
      container:
        image: python:3.10-slim
        command: [sh, -c]
        args:
          - pip install -r /src/requirements.txt && python /src/validate_data.py --input /tmp/input.csv --output /tmp/validated.csv
        volumeMounts:
          - name: scripts
            mountPath: /src
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
      volumes:
        - name: scripts
          configMap:
            name: ml-training-scripts

    # Step 3: Feature engineering
    - name: feature-engineering
      retryStrategy:
        limit: 2
        retryPolicy: "OnError"
      inputs:
        parameters:
          - name: target-column
        artifacts:
          - name: input-data
            path: /tmp/input.csv
      outputs:
        artifacts:
          - name: output-data
            path: /tmp/features.csv
          - name: scaler
            path: /tmp/features_scaler.joblib
            optional: true
      container:
        image: python:3.10-slim
        command: [sh, -c]
        args:
          - pip install -r /src/requirements.txt && python /src/feature_engineering.py --input /tmp/input.csv --output /tmp/features.csv --target "{{inputs.parameters.target-column}}"
        volumeMounts:
          - name: scripts
            mountPath: /src
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 512Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
      volumes:
        - name: scripts
          configMap:
            name: ml-training-scripts

    # Step 4: Train model with MLflow tracking
    - name: train-model
      retryStrategy:
        limit: 2
        retryPolicy: "OnError"
      inputs:
        parameters:
          - name: target-column
          - name: model-name
          - name: mlflow-tracking-uri
          - name: n-estimators
          - name: max-depth
          - name: test-size
          - name: random-state
        artifacts:
          - name: input-data
            path: /tmp/input.csv
          - name: scaler
            path: /tmp/scaler.joblib
            optional: true
      outputs:
        parameters:
          - name: run-id
            valueFrom:
              path: /tmp/run_id.txt
          - name: accuracy
            valueFrom:
              path: /tmp/accuracy.txt
        artifacts:
          - name: model
            path: /tmp/model.joblib
          - name: scaler
            path: /tmp/scaler.joblib
            optional: true
      container:
        image: python:3.10-slim
        command: [sh, -c]
        args:
          - |
            pip install -r /src/requirements.txt && \
            python /src/train_model.py \
              --input /tmp/input.csv \
              --model-output /tmp/model.joblib \
              --run-id-output /tmp/run_id.txt \
              --accuracy-output /tmp/accuracy.txt \
              --target "{{inputs.parameters.target-column}}" \
              --model-name "{{inputs.parameters.model-name}}" \
              --mlflow-uri "{{inputs.parameters.mlflow-tracking-uri}}" \
              --n-estimators "{{inputs.parameters.n-estimators}}" \
              --max-depth "{{inputs.parameters.max-depth}}" \
              --test-size "{{inputs.parameters.test-size}}" \
              --random-state "{{inputs.parameters.random-state}}"
        volumeMounts:
          - name: scripts
            mountPath: /src
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 2000m
            memory: 2Gi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
      volumes:
        - name: scripts
          configMap:
            name: ml-training-scripts

    # Step 5: Register model with MLflow alias
    - name: register-model
      retryStrategy:
        limit: 2
        retryPolicy: "OnError"
      inputs:
        parameters:
          - name: model-name
          - name: mlflow-tracking-uri
          - name: accuracy-threshold
          - name: model-alias
          - name: run-id
      container:
        image: python:3.10-slim
        command: [sh, -c]
        args:
          - |
            pip install -r /src/requirements.txt && \
            python /src/register_model.py \
              --model-name "{{inputs.parameters.model-name}}" \
              --mlflow-uri "{{inputs.parameters.mlflow-tracking-uri}}" \
              --threshold "{{inputs.parameters.accuracy-threshold}}" \
              --alias "{{inputs.parameters.model-alias}}" \
              --run-id "{{inputs.parameters.run-id}}"
        volumeMounts:
          - name: scripts
            mountPath: /src
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
      volumes:
        - name: scripts
          configMap:
            name: ml-training-scripts

---
# Workflow to run the training pipeline
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ml-training-
  namespace: argo
  labels:
    app: mlops
    pipeline: training
spec:
  workflowTemplateRef:
    name: ml-training-pipeline