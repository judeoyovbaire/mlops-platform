# Example InferenceService for local testing
# Uses public GCS bucket - no credentials required
# Works with RawDeployment mode (no Knative)
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: sklearn-iris
  namespace: mlops
  labels:
    app.kubernetes.io/part-of: mlops-platform
    environment: local
spec:
  predictor:
    model:
      modelFormat:
        name: sklearn
      runtime: kserve-sklearnserver
      # Public GCS bucket - no credentials required
      storageUri: gs://kfserving-examples/models/sklearn/1.0/model
      resources:
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 500m
          memory: 512Mi