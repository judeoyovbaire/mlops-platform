# Prometheus ServiceMonitors for MLOps Platform
# Requires prometheus-operator to be installed

---
# ServiceMonitor for MLflow
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: mlflow
  namespace: mlflow
  labels:
    app: mlflow
    release: prometheus
spec:
  selector:
    matchLabels:
      app: mlflow
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
  namespaceSelector:
    matchNames:
      - mlflow

---
# ServiceMonitor for KServe InferenceServices
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kserve-inference
  namespace: mlops
  labels:
    app: kserve
    release: prometheus
spec:
  selector:
    matchLabels:
      serving.kserve.io/inferenceservice: "true"
  endpoints:
    - port: http
      path: /metrics
      interval: 15s
  namespaceSelector:
    matchNames:
      - mlops

---
# PodMonitor for Argo Workflow runs
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: argo-workflows
  namespace: argo
  labels:
    app: argo-workflows
    release: prometheus
spec:
  selector:
    matchLabels:
      workflows.argoproj.io/workflow: ""
  podMetricsEndpoints:
    - port: metrics
      path: /metrics
      interval: 30s
  namespaceSelector:
    matchNames:
      - argo

---
# PrometheusRule for MLOps alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: mlops-alerts
  namespace: mlops
  labels:
    release: prometheus
spec:
  groups:
    - name: mlops.rules
      rules:
        # High inference latency
        - alert: HighInferenceLatency
          expr: |
            histogram_quantile(0.95,
              sum(rate(revision_request_latencies_bucket{namespace="mlops"}[5m])) by (le, revision_name)
            ) > 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High inference latency for {{ $labels.revision_name }}"
            description: "P95 latency is above 1 second for inference service {{ $labels.revision_name }}"

        # Inference service errors
        - alert: HighInferenceErrorRate
          expr: |
            sum(rate(revision_request_count{response_code!="200",namespace="mlops"}[5m])) by (revision_name)
            /
            sum(rate(revision_request_count{namespace="mlops"}[5m])) by (revision_name)
            > 0.05
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High error rate for {{ $labels.revision_name }}"
            description: "Error rate is above 5% for inference service {{ $labels.revision_name }}"

        # Model serving pod restarts
        - alert: InferenceServiceRestarts
          expr: |
            increase(kube_pod_container_status_restarts_total{namespace="mlops"}[1h]) > 3
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Multiple restarts for {{ $labels.pod }}"
            description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour"

        # MLflow tracking server down
        - alert: MLflowDown
          expr: |
            up{job="mlflow", namespace="mlflow"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "MLflow tracking server is down"
            description: "MLflow tracking server has been down for more than 2 minutes"

        # Argo Workflow failure rate
        - alert: HighWorkflowFailureRate
          expr: |
            sum(rate(argo_workflows_count{status="Failed"}[1h]))
            /
            sum(rate(argo_workflows_count[1h]))
            > 0.1
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "High Argo Workflow failure rate"
            description: "More than 10% of workflows are failing in the last hour"

        # GPU utilization low (cost optimization)
        - alert: LowGPUUtilization
          expr: |
            avg(DCGM_FI_DEV_GPU_UTIL{namespace="mlops"}) by (pod) < 20
          for: 30m
          labels:
            severity: info
          annotations:
            summary: "Low GPU utilization for {{ $labels.pod }}"
            description: "GPU utilization is below 20% for pod {{ $labels.pod }}. Consider scaling down."

        # GPU memory exhaustion
        - alert: HighGPUMemoryUsage
          expr: |
            avg(DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL * 100) by (pod) > 90
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High GPU memory usage for {{ $labels.pod }}"
            description: "GPU memory usage is above 90% for pod {{ $labels.pod }}. Risk of OOM."

        # Inference service not ready
        - alert: InferenceServiceNotReady
          expr: |
            kube_deployment_status_replicas_available{namespace="mlops"} < kube_deployment_spec_replicas{namespace="mlops"}
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Inference service {{ $labels.deployment }} not fully available"
            description: "{{ $labels.deployment }} has only {{ $value }} of {{ $labels.replicas }} replicas available"

        # Persistent Volume usage high
        - alert: HighPVUsage
          expr: |
            kubelet_volume_stats_used_bytes{namespace=~"mlflow|argo"} / kubelet_volume_stats_capacity_bytes * 100 > 85
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "High storage usage on {{ $labels.persistentvolumeclaim }}"
            description: "PVC {{ $labels.persistentvolumeclaim }} is {{ $value | printf \"%.1f\" }}% full"

        # Certificate expiring soon
        - alert: TLSCertificateExpiringSoon
          expr: |
            certmanager_certificate_expiration_timestamp_seconds - time() < 604800
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: "TLS certificate {{ $labels.name }} expiring soon"
            description: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} expires in less than 7 days"

        # Kyverno policy violations
        - alert: KyvernoPolicyViolations
          expr: |
            increase(kyverno_policy_results_total{rule_result="fail"}[1h]) > 0
          for: 5m
          labels:
            severity: info
          annotations:
            summary: "Kyverno policy violation detected"
            description: "Policy {{ $labels.policy_name }} has been violated. Check resources for compliance."

        # Data drift detected â€” triggers automated retraining via Argo Events
        - alert: DataDriftDetected
          expr: |
            model_data_drift_score{namespace="mlops"} > 0.15
          for: 10m
          labels:
            severity: warning
            automation: retrain
          annotations:
            summary: "Data drift detected for {{ $labels.model_name }}"
            description: "Drift score {{ $value | printf \"%.3f\" }} exceeds threshold 0.15 for model {{ $labels.model_name }}. Automated retraining will be triggered."

        # Model prediction queue depth (for batch inference)
        - alert: HighPredictionQueueDepth
          expr: |
            sum(revision_queue_depth{namespace="mlops"}) by (revision_name) > 100
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High queue depth for {{ $labels.revision_name }}"
            description: "Prediction queue depth is {{ $value }} for {{ $labels.revision_name }}. Consider scaling up."

---
# Grafana Dashboard ConfigMap (for auto-provisioning)
apiVersion: v1
kind: ConfigMap
metadata:
  name: mlops-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  mlops-overview.json: |
    {
      "annotations": {
        "list": []
      },
      "title": "MLOps Platform Overview",
      "uid": "mlops-overview",
      "version": 1,
      "panels": [
        {
          "title": "Inference Requests/sec",
          "type": "stat",
          "datasource": "Prometheus",
          "gridPos": {"h": 4, "w": 6, "x": 0, "y": 0},
          "targets": [
            {
              "expr": "sum(rate(revision_request_count{namespace=\"mlops\"}[5m]))",
              "legendFormat": "Requests/sec"
            }
          ]
        },
        {
          "title": "P95 Latency",
          "type": "stat",
          "datasource": "Prometheus",
          "gridPos": {"h": 4, "w": 6, "x": 6, "y": 0},
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum(rate(revision_request_latencies_bucket{namespace=\"mlops\"}[5m])) by (le))",
              "legendFormat": "P95 Latency"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s"
            }
          }
        },
        {
          "title": "Active Models",
          "type": "stat",
          "datasource": "Prometheus",
          "gridPos": {"h": 4, "w": 6, "x": 12, "y": 0},
          "targets": [
            {
              "expr": "count(kube_pod_info{namespace=\"mlops\", pod=~\".*predictor.*\"})",
              "legendFormat": "Models"
            }
          ]
        },
        {
          "title": "Error Rate",
          "type": "stat",
          "datasource": "Prometheus",
          "gridPos": {"h": 4, "w": 6, "x": 18, "y": 0},
          "targets": [
            {
              "expr": "sum(rate(revision_request_count{response_code!=\"200\",namespace=\"mlops\"}[5m])) / sum(rate(revision_request_count{namespace=\"mlops\"}[5m])) * 100",
              "legendFormat": "Error %"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"color": "green", "value": null},
                  {"color": "yellow", "value": 1},
                  {"color": "red", "value": 5}
                ]
              }
            }
          }
        },
        {
          "title": "Inference Latency by Model",
          "type": "timeseries",
          "datasource": "Prometheus",
          "gridPos": {"h": 8, "w": 12, "x": 0, "y": 4},
          "targets": [
            {
              "expr": "histogram_quantile(0.95, sum(rate(revision_request_latencies_bucket{namespace=\"mlops\"}[5m])) by (le, revision_name))",
              "legendFormat": "{{revision_name}}"
            }
          ]
        },
        {
          "title": "Request Rate by Model",
          "type": "timeseries",
          "datasource": "Prometheus",
          "gridPos": {"h": 8, "w": 12, "x": 12, "y": 4},
          "targets": [
            {
              "expr": "sum(rate(revision_request_count{namespace=\"mlops\"}[5m])) by (revision_name)",
              "legendFormat": "{{revision_name}}"
            }
          ]
        }
      ]
    }