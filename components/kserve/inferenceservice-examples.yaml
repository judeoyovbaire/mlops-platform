# KServe InferenceService Examples
# These use public pre-trained models to validate KServe functionality

# Basic sklearn model deployment
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: sklearn-iris
  namespace: mlops
spec:
  predictor:
    model:
      modelFormat:
        name: sklearn
      storageUri: gs://kfserving-examples/models/sklearn/1.0/model
      resources:
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 500m
          memory: 512Mi
---
# Gateway API configuration for external access (replaces deprecated Ingress)
# AWS Load Balancer Controller creates an ALB for this Gateway
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: kserve-gateway
  namespace: mlops
  annotations:
    # ALB configuration
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  gatewayClassName: amazon-alb
  listeners:
    - name: http
      protocol: HTTP
      port: 80
---
# HTTPRoute to route traffic to the sklearn-iris inference service
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: sklearn-iris-route
  namespace: mlops
spec:
  parentRefs:
    - name: kserve-gateway
      namespace: mlops
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: /
      backendRefs:
        - name: sklearn-iris-predictor
          port: 80