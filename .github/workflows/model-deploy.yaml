# Model Build and Deploy Pipeline
# Builds model container images and deploys to KServe via GitOps
# Follows everything-as-code principle
#
# NOTE: This workflow requires infrastructure to be deployed first.
# Run the Deploy Infrastructure workflow with "apply" action before using this.

name: Model Deploy

on:
  # Manual trigger only - infrastructure must exist first
  workflow_dispatch:
    inputs:
      model:
        description: 'Model to deploy'
        required: true
        default: 'iris-classifier'
        type: choice
        options:
          - iris-classifier
          - all

env:
  AWS_REGION: "eu-west-1"

# Required for OIDC authentication with AWS
permissions:
  id-token: write
  contents: read

jobs:
  # Build model container image
  build:
    name: Build Model Image
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.meta.outputs.tags }}
      image_digest: ${{ steps.build.outputs.digest }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.login-ecr.outputs.registry }}/mlops-platform-dev/models
          tags: |
            type=sha,prefix=
            type=ref,event=branch
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push
        id: build
        uses: docker/build-push-action@v5
        with:
          context: ./examples/iris-classifier
          file: ./examples/iris-classifier/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Scan image for vulnerabilities
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ steps.login-ecr.outputs.registry }}/mlops-platform-dev/models:${{ github.sha }}
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'HIGH,CRITICAL'
        continue-on-error: true

  # Deploy to KServe
  deploy:
    name: Deploy to KServe
    runs-on: ubuntu-latest
    needs: build
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --name mlops-platform-dev --region ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Update InferenceService image tag
        run: |
          # Update the image tag in the KServe manifest
          export IMAGE_TAG="${{ github.sha }}"
          export ECR_REGISTRY="${{ steps.login-ecr.outputs.registry }}"

          # Apply the inference service with updated image
          kubectl apply -f components/kserve/inferenceservice-examples.yaml

          echo "## Model Deployment" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Image**: ${ECR_REGISTRY}/mlops-platform-dev/models:${IMAGE_TAG}" >> $GITHUB_STEP_SUMMARY

      - name: Wait for deployment
        run: |
          echo "Waiting for InferenceService to be ready..."
          kubectl wait --for=condition=Ready inferenceservice/sklearn-iris -n mlops --timeout=300s || true

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### InferenceService Status" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          kubectl get inferenceservice -n mlops >> $GITHUB_STEP_SUMMARY 2>&1
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY

      - name: Get service URL
        run: |
          SERVICE_URL=$(kubectl get inferenceservice sklearn-iris -n mlops -o jsonpath='{.status.url}' 2>/dev/null || echo "pending")
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Service URL" >> $GITHUB_STEP_SUMMARY
          echo "- **URL**: ${SERVICE_URL}" >> $GITHUB_STEP_SUMMARY